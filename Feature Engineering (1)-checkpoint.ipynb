{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a9d24-fcb2-42bb-90fd-c8afab4c25e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Thrashers\n",
    "# Dan Schumacher hdd249, Austin Thrash, Justin Pons, Jordan Dever xfd461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "efaf32ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Technology: 0.34\n",
      "Cohen's Kappa Transportation: 0.47\n",
      "Cohen's Kappa Utility: 0.39\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "# Import Annotations\n",
    "\n",
    "#CSV Columns\n",
    "#Comment[0]\tTech_Scav[1]\tTech_Det[2]\tTech_Adj[3]\tTrans_Scav[4]\tTrans_Det[5]\tTrans_Adj[6]\tUtil_Scav[7]\tUtil_Det[8]\tUtil_Adj[9]\n",
    "inData = []\n",
    "with open('./data/Adjudication.csv', 'r', encoding=\"utf-8\") as iFile:\n",
    "    file = csv.reader(iFile)\n",
    "    for row in file:\n",
    "        inData.append(row)\n",
    "\n",
    "# Calculate Agreement\n",
    "#CSV Columns\n",
    "\n",
    "Tech_Scav = []\n",
    "Tech_Det = []\n",
    "Trans_Scav = []\n",
    "Trans_Det = []\n",
    "Util_Scav = []\n",
    "Util_Det = []\n",
    "\n",
    "for row in inData:\n",
    "    Tech_Scav.append(row[1])\n",
    "    Tech_Det.append(row[2])\n",
    "    Trans_Scav.append(row[4])\n",
    "    Trans_Det.append(row[5])\n",
    "    Util_Scav.append(row[7])\n",
    "    Util_Det.append(row[8])\n",
    "TechCohen = cohen_kappa_score(Tech_Scav,Tech_Det)\n",
    "TransCohen = cohen_kappa_score(Trans_Scav,Trans_Det)\n",
    "UtilCohen = cohen_kappa_score(Util_Scav,Util_Det)\n",
    "print(\"Cohen's Kappa Technology: {:.2f}\\nCohen's Kappa Transportation: {:.2f}\\nCohen's Kappa Utility: {:.2f}\".format(TechCohen,TransCohen,UtilCohen))\n",
    "\n",
    "\n",
    "# Create y\n",
    "y = []\n",
    "for row in inData:\n",
    "    temp = []\n",
    "    temp.append(row[3])\n",
    "    temp.append(row[6])\n",
    "    temp.append(row[9])\n",
    "    y.append(temp)\n",
    "\n",
    "\n",
    "# Create X\n",
    "X = []\n",
    "for row in inData:\n",
    "    X.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "99d62ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "reddit_post = []\n",
    "tech = []\n",
    "trans = []\n",
    "util = []\n",
    "\n",
    "# Grab data from file\n",
    "with open('./data/Adjudication.csv', 'r', encoding=\"utf-8\") as iFile:\n",
    "    file = csv.reader(iFile)\n",
    "    for row in file:\n",
    "        all_rows.append(row)\n",
    "\n",
    "# make lists of unique info\n",
    "for row in all_rows:\n",
    "    reddit_post.append(row[0])\n",
    "    tech.append(row[3])\n",
    "    trans.append(row[6])\n",
    "    util.append(row[9])\n",
    "    \n",
    "data_dict = {'reddit_post':reddit_post,\n",
    "            'tech':tech,\n",
    "            'trans':trans,\n",
    "            'util':util}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9c6aa38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechLexiconClassifier():\n",
    "    def __init__(self):\n",
    "        self.tech_words = set()\n",
    "        with open('./data/technology_lexicon.txt', encoding='utf-8') as iFile:\n",
    "            for row in iFile:\n",
    "                words = row.strip().split()\n",
    "                self.tech_words.update(map(str.lower, words))\n",
    "                \n",
    "\n",
    "    def count_up(self, reddit_post):\n",
    "        num_tech_words = 0\n",
    "        num_non_tech_words = 0\n",
    "\n",
    "        # Split the reddit_post into individual words\n",
    "        words_in_post = reddit_post.lower().split()\n",
    "\n",
    "        for word in words_in_post:\n",
    "            if word in self.tech_words:\n",
    "                num_tech_words += 1\n",
    "            else:\n",
    "                num_non_tech_words += 1\n",
    "\n",
    "        return [num_tech_words, num_non_tech_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fa5cff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransLexiconClassifier():\n",
    "    def __init__(self):\n",
    "        self.trans_words = set()\n",
    "        with open('./data/transportation_lexicon.txt', encoding='utf-8') as iFile:\n",
    "            for row in iFile:\n",
    "                words = row.strip().split()\n",
    "                self.trans_words.update(map(str.lower, words))\n",
    "\n",
    "    def count_up(self, reddit_post):\n",
    "        num_trans_words = 0\n",
    "        num_non_trans_words = 0\n",
    "\n",
    "        # Split the reddit_post into individual words\n",
    "        words_in_post = reddit_post.lower().split()\n",
    "\n",
    "        for word in words_in_post:\n",
    "            if word in self.trans_words:\n",
    "                num_trans_words += 1\n",
    "            else:\n",
    "                num_non_trans_words += 1\n",
    "        \n",
    "        return [num_trans_words, num_non_trans_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c32ad7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtilLexiconClassifier():\n",
    "    def __init__(self):\n",
    "        self.util_words = set()\n",
    "        with open('./data/utilities_lexicon.txt', encoding='utf-8') as iFile:\n",
    "            for row in iFile:\n",
    "                words = row.strip().split()\n",
    "                self.util_words.update(map(str.lower, words))\n",
    "\n",
    "    def count_up(self, reddit_post):\n",
    "        num_util_words = 0\n",
    "        num_non_util_words = 0\n",
    "\n",
    "        # Split the reddit_post into individual words\n",
    "        words_in_post = reddit_post.lower().split()\n",
    "\n",
    "        for word in words_in_post:\n",
    "            if word in self.util_words:\n",
    "                num_util_words += 1\n",
    "            else:\n",
    "                num_non_util_words += 1\n",
    "\n",
    "        return [num_util_words, num_non_util_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2c94e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_lc = TechLexiconClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "81f22e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_lc = TransLexiconClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "42c50b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "util_lc = UtilLexiconClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6a6f5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_counts = []\n",
    "for reddit_post in data_dict['reddit_post']:\n",
    "    tech_counts.append(tech_lc.count_up(reddit_post))\n",
    "\n",
    "data_dict['tech_text_counts'] = tech_counts\n",
    "###########################################################\n",
    "trans_counts = []\n",
    "for reddit_post in data_dict['reddit_post']:\n",
    "    trans_counts.append(trans_lc.count_up(reddit_post))\n",
    "\n",
    "data_dict['trans_text_counts'] = trans_counts\n",
    "###########################################################\n",
    "util_counts = []\n",
    "for reddit_post in data_dict['reddit_post']:\n",
    "    util_counts.append(util_lc.count_up(reddit_post))\n",
    "\n",
    "data_dict['util_text_counts'] = util_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "88037b3a-ac18-4ffd-a905-883290ee5ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code Block Description:\n",
    "\n",
    "Gets validation F1 score, precision score, recall score, and macro F1 score \n",
    "using the Technology Lexicon Classifier created prior.\n",
    "\n",
    "- Variables:\n",
    "    tech_validation\n",
    "    tech_accuracy\n",
    "    tech_precision\n",
    "    tech_recall\n",
    "    tech_f1\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score\n",
    "\n",
    "# Filtering out warnings when modeling, warnings are mainly about future changes to the library\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Getting annotations for tech, using X and y variable; it looks cleaner to use\n",
    "y_tech = [row[0] for row in y]\n",
    "\n",
    "# Splitting data for tech, passing random_state arguement for reproducibility\n",
    "X_txt_train, X_txt_test, y_train, y_test = train_test_split(X, y_tech, test_size=0.2, random_state=42)\n",
    "\n",
    "# Emplty list for lexicon feature counts\n",
    "X_train_lexicon_features = []\n",
    "X_test_lexicon_features = [] \n",
    "\n",
    "# Loop over X_txt_train and X_txt_test to get feature counts of newly split data\n",
    "for i in X_txt_train:\n",
    "# Use the tech lexicon to get feature counts\n",
    "    feature_counts = tech_lc.count_up(i)\n",
    "    X_train_lexicon_features.append(feature_counts)\n",
    "\n",
    "for i in X_txt_test:\n",
    "# Use the tech lexicon to get feature counts\n",
    "    feature_counts = tech_lc.count_up(i)\n",
    "    X_test_lexicon_features.append(feature_counts)\n",
    "\n",
    "# Convert X_txt_train and X_txt_test to matricies of numbers \n",
    "countVec = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "X_txt_train = countVec.fit_transform(X_txt_train)\n",
    "X_txt_test = countVec.transform(X_txt_test)\n",
    "\n",
    "# Using hstack to convert X_train_lexicon_features and X_test_lexicon_features to numpy arrays\n",
    "X_train = hstack([X_train_lexicon_features, X_txt_train])\n",
    "X_test = hstack([X_test_lexicon_features, X_txt_test])\n",
    "\n",
    "# Initialize the classifier LinearSVC \n",
    "pipe = Pipeline([('skb', SelectKBest()),\n",
    "                 ('clf', LinearSVC(random_state=42))])\n",
    "\n",
    "# Create the params\n",
    "params = {'clf__C': [0.01,0.1,1, 10, 100, 1000],\n",
    "         'skb__k': [10,100,1000,10000,20000]}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "GSearch = GridSearchCV(pipe, params, scoring = 'f1_macro', cv = 5)\n",
    "\n",
    "# fitting the model\n",
    "GSearch.fit(X_train, y_train)\n",
    "\n",
    "tech_validation = GSearch.best_score_\n",
    "\n",
    "svm_lex_test_predictions = GSearch.predict(X_test)\n",
    "\n",
    "tech_accuracy = accuracy_score(y_test, svm_lex_test_predictions)\n",
    "tech_precision = precision_score(y_test, svm_lex_test_predictions, average='macro') # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "tech_recall = recall_score(y_test, svm_lex_test_predictions, average='macro')\n",
    "tech_f1 = f1_score(y_test, svm_lex_test_predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5b3b9051-ad3a-41f1-83fe-68b5cd2196c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code Block Description:\n",
    "\n",
    "Gets validation F1 score, precision score, recall score, and macro F1 score \n",
    "using the Utility Lexicon Classifier created prior.\n",
    "\n",
    "- Variables:\n",
    "    util_validation\n",
    "    util_accuracy\n",
    "    util_precision\n",
    "    util_recall\n",
    "    util_f1\n",
    "\"\"\"\n",
    "# Filtering out warnings when modeling, warnings are mainly about future changes to the library\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Getting annotations for utilities, using X and y variable; it looks cleaner to use\n",
    "y_util = [row[2] for row in y]\n",
    "\n",
    "# Splitting data for utilities\n",
    "X_txt_train, X_txt_test, y_train, y_test = train_test_split(X, y_util, test_size=0.2, random_state=42)\n",
    "\n",
    "# Emplty list for lexicon feature counts\n",
    "X_train_lexicon_features = []\n",
    "X_test_lexicon_features = [] \n",
    "\n",
    "# Loop over X_txt_train and X_txt_test to get feature counts of newly split data\n",
    "for i in X_txt_train:\n",
    "# Use the utilities lexicon to get feature counts\n",
    "    feature_counts = util_lc.count_up(i)\n",
    "    X_train_lexicon_features.append(feature_counts)\n",
    "\n",
    "for i in X_txt_test:\n",
    "# Use the utilities lexicon to get feature counts\n",
    "    feature_counts = util_lc.count_up(i)\n",
    "    X_test_lexicon_features.append(feature_counts)\n",
    "\n",
    "# Convert X_txt_train and X_txt_test to matricies of numbers \n",
    "countVec = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "X_txt_train = countVec.fit_transform(X_txt_train)\n",
    "X_txt_test = countVec.transform(X_txt_test)\n",
    "\n",
    "# Using hstack to convert X_train_lexicon_features and X_test_lexicon_features to numpy arrays\n",
    "X_train = hstack([X_train_lexicon_features, X_txt_train])\n",
    "X_test = hstack([X_test_lexicon_features, X_txt_test])\n",
    "\n",
    "# Initialize the classifier LinearSVC \n",
    "pipe = Pipeline([('skb', SelectKBest()),\n",
    "                 ('clf', LinearSVC(random_state=42))])\n",
    "\n",
    "# Create the params\n",
    "params = {'clf__C': [0.01,0.1,1, 10, 100, 1000],\n",
    "         'skb__k': [10,100,1000,10000,20000]}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "GSearch = GridSearchCV(pipe, params, scoring = 'f1_macro', cv = 5)\n",
    "\n",
    "# fitting the model\n",
    "GSearch.fit(X_train, y_train)\n",
    "\n",
    "util_validation = GSearch.best_score_\n",
    "\n",
    "svm_lex_test_predictions = GSearch.predict(X_test)\n",
    "\n",
    "util_accuracy = accuracy_score(y_test, svm_lex_test_predictions)\n",
    "util_precision = precision_score(y_test, svm_lex_test_predictions, average='macro') # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "util_recall = recall_score(y_test, svm_lex_test_predictions, average='macro')\n",
    "util_f1 = f1_score(y_test, svm_lex_test_predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c193efab-6224-41e2-9615-668acd80c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code Block Description:\n",
    "\n",
    "Gets validation F1 score, precision score, recall score, and macro F1 score \n",
    "using the Transportation Lexicon Classifier created prior.\n",
    "\n",
    "- Variables:\n",
    "    trans_validation\n",
    "    trans_accuracy\n",
    "    trans_precision\n",
    "    trans_recall\n",
    "    trans_f1\n",
    "\"\"\"\n",
    "# Filtering out warnings when modeling, warnings are mainly about future changes to the library\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Getting annotations for transportation, using X and y variable; it looks cleaner to use\n",
    "y_trans = [row[1] for row in y]\n",
    "\n",
    "# Splitting data for transportation\n",
    "X_txt_train, X_txt_test, y_train, y_test = train_test_split(X, y_trans, test_size=0.2, random_state=42)\n",
    "\n",
    "# Emplty list for lexicon feature counts\n",
    "X_train_lexicon_features = []\n",
    "X_test_lexicon_features = [] \n",
    "\n",
    "# Loop over X_txt_train and X_txt_test to get feature counts of newly split data\n",
    "for i in X_txt_train:\n",
    "# Use the transportation lexicon to get feature counts\n",
    "    feature_counts = trans_lc.count_up(i)\n",
    "    X_train_lexicon_features.append(feature_counts)\n",
    "\n",
    "for i in X_txt_test:\n",
    "# Use the transportation lexicon to get feature counts\n",
    "    feature_counts = trans_lc.count_up(i)\n",
    "    X_test_lexicon_features.append(feature_counts)\n",
    "\n",
    "# Convert X_txt_train and X_txt_test to matricies of numbers \n",
    "countVec = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "X_txt_train = countVec.fit_transform(X_txt_train)\n",
    "X_txt_test = countVec.transform(X_txt_test)\n",
    "\n",
    "# Using hstack to convert X_train_lexicon_features and X_test_lexicon_features to numpy arrays\n",
    "X_train = hstack([X_train_lexicon_features, X_txt_train])\n",
    "X_test = hstack([X_test_lexicon_features, X_txt_test])\n",
    "\n",
    "# Initialize the classifier LinearSVC \n",
    "pipe = Pipeline([('skb', SelectKBest()),\n",
    "                 ('clf', LinearSVC(random_state=42))])\n",
    "\n",
    "# Create the params\n",
    "params = {'clf__C': [0.01,0.1,1, 10, 100, 1000],\n",
    "         'skb__k': [10,100,1000,10000,20000]}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "GSearch = GridSearchCV(pipe, params, scoring = 'f1_macro', cv = 5)\n",
    "\n",
    "# fitting the model\n",
    "GSearch.fit(X_train, y_train)\n",
    "\n",
    "trans_validation = GSearch.best_score_\n",
    "\n",
    "svm_lex_test_predictions = GSearch.predict(X_test)\n",
    "\n",
    "trans_accuracy = accuracy_score(y_test, svm_lex_test_predictions)\n",
    "trans_precision = precision_score(y_test, svm_lex_test_predictions, average='macro') # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "trans_recall = recall_score(y_test, svm_lex_test_predictions, average='macro')\n",
    "trans_f1 = f1_score(y_test, svm_lex_test_predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5cfcb4fe-3b89-4109-98c0-4b9be6d2dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology Model Results:\n",
      "\t- Validation F1: 0.7011\n",
      "\t- Accuracy: 0.7650\n",
      "\t- Precision: 0.7406\n",
      "\t- Recall: 0.6866\n",
      "\t- F1: 0.7002\n"
     ]
    }
   ],
   "source": [
    "# Printing scores of all the models\n",
    "print(\"Technology Model Results:\")\n",
    "print(\"\\t- Validation F1: {:.4f}\".format(tech_validation))\n",
    "print(\"\\t- Accuracy: {:.4f}\".format(tech_accuracy))\n",
    "print(\"\\t- Precision: {:.4f}\".format(tech_precision))\n",
    "print(\"\\t- Recall: {:.4f}\".format(tech_recall))\n",
    "print(\"\\t- F1: {:.4f}\".format(tech_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6404c59d-4782-4fed-bf28-898f3f47772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility Model Results:\n",
      "\t- Validation F1: 0.6659\n",
      "\t- Accuracy: 0.9850\n",
      "\t- Precision: 0.9924\n",
      "\t- Recall: 0.7000\n",
      "\t- F1: 0.7819\n"
     ]
    }
   ],
   "source": [
    "print(\"Utility Model Results:\")\n",
    "print(\"\\t- Validation F1: {:.4f}\".format(util_validation))\n",
    "print(\"\\t- Accuracy: {:.4f}\".format(util_accuracy))\n",
    "print(\"\\t- Precision: {:.4f}\".format(util_precision))\n",
    "print(\"\\t- Recall: {:.4f}\".format(util_recall))\n",
    "print(\"\\t- F1: {:.4f}\".format(util_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8ec4e83d-1f27-4c6a-b043-a78d32a6350d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transportation Model Results:\n",
      "\t- Validation F1: 0.7371\n",
      "\t- Accuracy: 0.9350\n",
      "\t- Precision: 0.8072\n",
      "\t- Recall: 0.6793\n",
      "\t- F1: 0.7227\n"
     ]
    }
   ],
   "source": [
    "print(\"Transportation Model Results:\")\n",
    "print(\"\\t- Validation F1: {:.4f}\".format(trans_validation))\n",
    "print(\"\\t- Accuracy: {:.4f}\".format(trans_accuracy))\n",
    "print(\"\\t- Precision: {:.4f}\".format(trans_precision))\n",
    "print(\"\\t- Recall: {:.4f}\".format(trans_recall))\n",
    "print(\"\\t- F1: {:.4f}\".format(trans_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367075d4-23dd-4ac4-82f4-bcd01de87035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
