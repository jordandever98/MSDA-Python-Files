{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf686b-2b77-479b-b380-85b8be06c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES\n",
    "\n",
    "# MIDTERM\n",
    "\n",
    "## Will be posted on Tuesday.\n",
    "## Will be due Friday, Dec 1\n",
    "## \"MAIN\" topics will be everything from first Midterm until now\n",
    "## The exam is cumulitive. Expect at least 2 programming questions where you need to write code and 2 regex questions.'\n",
    "## 2 hours to complete. Same format as Exam 1\n",
    "# Everything will be on material after exam 1 - maybe a couple questions that arent regex\n",
    "## We will have Q/A next Tuesday.\n",
    "\n",
    "# Pipelines\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "\n",
    "# Feature Transformation/Selection\n",
    "\n",
    "## MinMaxScaler\n",
    "\n",
    "## StandardScaler\n",
    "\n",
    "## Normalizer\n",
    "\n",
    "## SelectKBest\n",
    "\n",
    "## PCA\n",
    "\n",
    "# Missing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26c266-ccd6-4b77-9f62-667fe7ed9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminders\n",
    "\n",
    "# Final Project\n",
    "1. Ajudication (assess how well annotators did and change if needed - even if they agreed)\n",
    "2. Modeling (3 types of features - do PS3 on it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8cf4a-2b91-472a-970a-8e76a9b45ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Basic Coding Concepts and FileIO\n",
    "floats ints strs lists sets dicts\n",
    "if elif else and not ! in or\n",
    "for while Range()\n",
    "#OOP\n",
    "Zoom into 50 lines of code and ignore the 999950 lines used to create it\n",
    "Methods vs Functions \n",
    "Method is a function contained within a class and objs constructed from the class\n",
    "Objs have state and attributes (class & instance)\n",
    "A rabbit object:\n",
    "    Has data how hungy, frightened, and where it is\n",
    "    And Methods: eat, hide, dig\n",
    "class Rabbit:\n",
    "    x=0\n",
    "    def party(self):\n",
    "        self.x += 1\n",
    "Inheritance - have 2 classes, one can be parent and child class\n",
    "then inherit parts of the parent class and access things that arent defined in child class\n",
    "\n",
    "Maybe 2 questions each for the stuff listed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15508d-d778-4d8f-8139-208b65f9e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine Learning (Feature Engineering)\n",
    "- Data Annotation\n",
    "- Feature Engineering\n",
    "- Model Selection and Evaluation\n",
    "\n",
    "How do we rep our data?\n",
    "Data Object reps an entity - Customers, Students,Prof,Cources,Tweets,Images,Genes<Drufs,Procedures\n",
    "Attribute is a data field (synonyms dimension/feature/variable)\n",
    "Types of Attributes:\n",
    "    Nominal - hair color\n",
    "    Binary (Boolean) -  yes no\n",
    "    Ordinal - scale\n",
    "    Numeric - numbers\n",
    "# Predict Customer Churn\n",
    "- Credit Score numeric 600\n",
    "- Location/State - Categorical (KY, OH, TX)\n",
    "- Salary - numeric\n",
    "- Landline? y/n binary\n",
    "- Coffee (tall,grande,venti) ordinal\n",
    "\n",
    "TX\n",
    "[600,1,0,0,50000,0,1,0,0] -  REVIEW LAST QUIZ FOR THIS\n",
    "One Hot Encoding\n",
    "\n",
    "Model Evalution\n",
    "- Model Selection\n",
    "Estimate performance of diff models in order to choose best one\n",
    "\n",
    "- Model Assessment\n",
    "What types of performance should we expect to get?\n",
    "\n",
    "3 Way Split is the most simple way to do Selection Assessment\n",
    "Train Validation Test Set\n",
    "Can NOT further tune model\n",
    "Validation data is likely to be fitted to and biasing our model (26min)\n",
    "\n",
    "K-Fold Cross-Validation\n",
    "For Each of the K experiments, use K-1 folds for training and the remaining for testing\n",
    "Great for small datasets\n",
    "\n",
    "How many folds are needed?\n",
    "With a large num of folds:\n",
    "    The bias of true error rate is small\n",
    "    Variance of true error tate will be large\n",
    "    Large comp time\n",
    "Small number of folds:\n",
    "    Comp time reduced\n",
    "    Variance of estimator will be small\n",
    "    The bias of estimator will be large (more conservation or higher than true error rate)\n",
    "In Practice, the choice of number o f folds depends on the size of the data set\n",
    "- More large datasets 3-fold CV is accurate for small datasets, leave one out\n",
    "Common choice is k=10\n",
    "\n",
    "\n",
    "Annotation Pipeline\n",
    "-Model & Guidelines\n",
    "-Annotate\n",
    "-Evaluate\n",
    "-Revise\n",
    "1. Determine what to annotate\n",
    "2. Formalize the instructions\n",
    "3. Perform pilot annotation\n",
    "4 Annotate the Data\n",
    "5. Release or keep adjusting the data\n",
    "Our goal: Define a very consitent task for annotators to follow so the model can learn something\n",
    "\n",
    "Adjudication Phase\n",
    "Process of deciding on a single annotation for a piece of text, using info about the independent annotations\n",
    "Can be time consuming\n",
    "Does NOT need to be identical with the primary annotation (both can be wrong by chance)\n",
    "\n",
    "Measure agreement\n",
    "Fleiss kappa (2+ Annotators)\n",
    "K = (Po - Pe) / (1-Pe)\n",
    "Po = Observed agreement\n",
    "Pe = Expected agreement\n",
    "\n",
    "Cohens Kappa only works with 2 annotators\n",
    "Kripendorffs Alpha\n",
    "we can calculate the cohens kappa for each column / category\n",
    "we can also see if agreement scores differ per category and then would techically change our guidelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bec51c3-dc00-4e72-9598-90cbec7896ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "grpA_tech = ['yes','no','yes','no']\n",
    "grpB_tech = ['yes','yes','yes','no']\n",
    "\n",
    "cohen_kappa_score(grpA_tech, grpB_tech)\n",
    "# .6 is good for sentiment analysis\n",
    "# .8 should be good for agreement scores\n",
    "# Kind of like a scale but it really depends on the task for good kappa score\n",
    "\n",
    "#BE ABLE TO FILL IN COHEN'S & FLEISS KAPPA TABLES (52min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99823ea-47b9-4680-bcbe-55921de46588",
   "metadata": {},
   "outputs": [],
   "source": [
    "Geometric Intution of Feature Selection and PCA\n",
    "Have 2 dimensions (2 features)\n",
    "Feature selection - choose a feature to keep and drop other\n",
    "Then Project points onto the x-axis (if we chose to keep x-axis)\n",
    "\n",
    "When do we want to do feature selection?\n",
    "If we have a lot of noise in our data and we want to reduce the noise\n",
    "Can assess this and see how much % of the features we need to remove\n",
    "Its not very common for feature selection to improve model\n",
    "Its more common to do it if we have memory constraints or if we need the model to run faster\n",
    "We dont have to restrict ourselves to picking either the x or y axis, we can create the best line to project to\n",
    "Feature Transforming\n",
    "PCA - Chooses a dimension that maximizes the variance of out data\n",
    "Suppervised methods, supervised pca\n",
    "\n",
    "PCA doesnt use any info about class labels\n",
    "Unsupervised dimensionaliy redux\n",
    "How do we choose which PCA axis to project on? # Most likely to see on the test\n",
    "The one that will maximize the width / variance of our data\n",
    "\n",
    "Types of Missing Data\n",
    "Missing Completely at Random (MCAR):\n",
    "    P(missing) is unrelated to process under study\n",
    "Missing at Random (MAR):\n",
    "    P(m) depends on obs data\n",
    "    If it rained we know that our sensor data will be lost\n",
    "Missing Not at Random (MNAR):\n",
    "    P(m) depends on both obs and unobs data\n",
    "If we alter the data we may bias our data\n",
    "\n",
    "Complete Case Analysis\n",
    "Delete all rows with any missing value at all\n",
    "Easiest way to handle missing data\n",
    "\n",
    "Mode\n",
    "Imputation of each feature with missing values\n",
    "Categorical features use mode \n",
    "Numerical features use the average or median \n",
    "We can also use data that is not missing to be able to predict a MV\n",
    "We would need to train a model to learn what value should be there and impute it in\n",
    "At the end of the day, the most important part is improving model performance and choosing that one\n",
    "There also be systematic errors in our data because like for WA banks cant have your age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c9bba-8a41-4843-b873-51586e6fd3d8",
   "metadata": {},
   "source": [
    "# Exercise 1: Missing Data\n",
    "\n",
    "Access to safe drinking-water is essential to health, a basic human right and a component of effective policy for health protection. This is important as a health and development issue at a national, regional and local level. In some regions, it has been shown that investments in water supply and sanitation can yield a net economic benefit, since the reductions in adverse health effects and health care costs outweigh the costs of undertaking the interventions. Yet, water drinkability is measured by multiple factors, many of which can not be used alone. Hence, this exercise we will develop methods of predicting whether water is drinkable based on variuos measured characteristics.\n",
    "\n",
    "**YOUR TASK:** There is missing data in the csv file. For each column, calculate how many missing values, i.e., count the number of ```np.nan``` values.\n",
    "\n",
    "**TIME**: 10 Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d1dea7-cead-41fa-880f-256a207a5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "X_dict = []\n",
    "y = []\n",
    "with open('water_potability_missing.csv') as iFile:\n",
    "    iCSV = csv.reader(iFile, delimiter=',')\n",
    "    header = next(iCSV)\n",
    "    for row in iCSV:\n",
    "        data = {}\n",
    "        for f, x in zip(header[:-1], row[:-1]):\n",
    "            if x != '':\n",
    "                data[f] = float(x)\n",
    "            else:\n",
    "                data[f] = np.nan\n",
    "        X_dict.append(data)\n",
    "        y.append(int(row[-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165ce519-afa6-4c6a-b2e8-037e49c433a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ph': nan,\n",
       " 'Hardness': 204.8904554713363,\n",
       " 'Solids': 20791.318980747026,\n",
       " 'Chloramines': 7.300211873184757,\n",
       " 'Sulfate': 368.51644134980336,\n",
       " 'Conductivity': 564.3086541722439,\n",
       " 'Organic_carbon': 10.3797830780847,\n",
       " 'Trihalomethanes': 86.9909704615088,\n",
       " 'Turbidity': 2.9631353806316407}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1ab9ba-1a44-4b7d-99ac-6c395fb21d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce12ac47-3a3a-467f-983c-5dc36db80db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "######################\n",
    "# Split dataset into a training and testing portion here.\n",
    "\n",
    "X_train_dict, X_test_dict, y_train, y_test = train_test_split(X_dict, y, test_size=0.2) # Replace the ... with your code to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843bb03d-03e6-433a-874d-16befc2a358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2620 656\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b956315-3e6e-4b97-b48d-9d0cf7cd619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates the matrix from each list of dictionaries.  You do NOT need to edit this.\n",
    "vec = DictVectorizer(sparse=False)\n",
    "\n",
    "X_train = vec.fit_transform(X_train_dict)\n",
    "X_test = vec.transform(X_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0d42615-1c85-4e5d-98cf-60fe0010da5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chloramines',\n",
       " 'Conductivity',\n",
       " 'Hardness',\n",
       " 'Organic_carbon',\n",
       " 'Solids',\n",
       " 'Sulfate',\n",
       " 'Trihalomethanes',\n",
       " 'Turbidity',\n",
       " 'ph']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3910c2fa-d0cf-445a-91a7-fc9c5ec392e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2620, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b31f4fd5-ef0e-4266-860d-6ad6b6441dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 9)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE CODE TO ANSWER QUESTIONS HERE. HINT: play with np.isnan(MATRIX_HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e8a4c-529c-4e50-b941-e5184f46bf05",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 2: Model Selection\n",
    "\n",
    "The data has been split into a training and test dataset. Now, your task is to train a machine learning classifier on the dataset. Specifically, we are going to train the Random classifier in scikit-learn. Your task is to evaluate three methods of handling missing data scikit-learn's SimpleImputer: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer\n",
    "\n",
    "1. mean\n",
    "2. median\n",
    "3. constant\n",
    "    1. Experiment with setting all missing values to zero.\n",
    "    \n",
    "Report the training F1 and cross-validation F1 in the following table:\n",
    "\n",
    "|Model|Impute Strategy| Training F1 | Validation F1|\n",
    "|----|----|----|----|\n",
    "|Random Forest| mean| SCORE HERE | SCORE HERE |\n",
    "Random Forest|median | SCORE HERE | SCORE HERE |\n",
    "Random Forest|constant| SCORE HERE | SCORE HERE |\n",
    "\n",
    "Finally, answer the questions: Is the training F1 higher then the validation F1? Why or why not?\n",
    "\n",
    "**TIME**: 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcff99-159a-484a-ab9b-75324a918928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "####################################################\n",
    "\n",
    "# Write code to use SimpleImputer here. (See documentation)\n",
    "\n",
    "imputer = SimpleImputer(...) # MODIFY THIS LINE\n",
    "\n",
    "X_train_impute = imputer.fit_transform(X_train)\n",
    "X_test_impute = imputer.transform(X_test)\n",
    "\n",
    "####################################################\n",
    "\n",
    "params = {'n_estimators': [50, 100, 250, 500]}\n",
    "\n",
    "base_clf = RandomForestClassifier()\n",
    "\n",
    "clf = GridSearchCV(base_clf, params, cv = 5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_impute, y_train)\n",
    "\n",
    "preds = clf.predict(X_train_impute)\n",
    "\n",
    "f1 = f1_score(y_train, preds)\n",
    "print(\"Best Parameters:\", clf.best_params_)\n",
    "print(f\"Training F1: {f1:.4f}\") # This is the \"Training F1\"\n",
    "print(f\"Validation F1: {clf.best_score_:.4f}\") # This is the Validation F1 from the cross-validation procedure in GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb41d1-cb78-4580-b146-5f42588ce695",
   "metadata": {},
   "source": [
    "# Exercise 3: Feature transformations and More Model Selection\n",
    "\n",
    "We also want to experiment with the the SVC classifier and different feature transformaiton methods. For this task complete the following:\n",
    "\n",
    "1. Apply the StandardScaler and MinMaxScaler to the to X_train and X_test (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html, and https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "2. Apply the SimpleImputer() with a strategy where strategy is set to \"constant\" and the fill_value is set to 0.\n",
    "\n",
    "|Model|Impute Strategy | Normalization Method | Training F1 | Validation F1|\n",
    "|----|----|----|----|----|\n",
    "|SVC| mean| StandardScaler | SCORE HERE | SCORE HERE |\n",
    "|SVC|median |  StandardScaler | SCORE HERE | SCORE HERE |\n",
    "|SVC|constant|  StandardScaler | SCORE HERE | SCORE HERE |\n",
    "|SVC| mean| MinMaxScaler | SCORE HERE | SCORE HERE |\n",
    "|SVC|median |  MinMaxScaler | SCORE HERE | SCORE HERE |\n",
    "|SVC|constant|  MinMaxScaler | SCORE HERE | SCORE HERE |\n",
    "\n",
    "\n",
    "What combination gives the best validation results?\n",
    "\n",
    "**TIME**: 10 Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc99c3-ad6a-40cf-a66a-f05ac6a3ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "####################################################\n",
    "\n",
    "# Write code to use SimpleImputer here. (See documentation)\n",
    "\n",
    "imputer = SimpleImputer(...) # MODIFY THIS LINE\n",
    "\n",
    "X_train_impute = imputer.fit_transform(X_train)\n",
    "X_test_impute = imputer.transform(X_test)\n",
    "\n",
    "####################################################\n",
    "\n",
    "####################################################\n",
    "\n",
    "# Write code to \"normalize\" or \"scale\" data here.\n",
    "\n",
    "scale =  # MODIFY THIS LINE (instantiate StandardScaler or MinMaxScaler here)\n",
    "\n",
    "X_train_impute_scale = scale.fit_transform(X_train_impute)\n",
    "X_test_impute_scale = scale.transform(X_test_impute)\n",
    "\n",
    "####################################################\n",
    "\n",
    "params = {'C': [.001, .01, .1, 1., 10., 100., 1000.]}\n",
    "\n",
    "base_clf = SVC()\n",
    "\n",
    "clf = GridSearchCV(base_clf, params, cv = 5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train_impute_scale, y_train)\n",
    "\n",
    "preds = clf.predict(X_train_impute_scale)\n",
    "\n",
    "f1 = f1_score(y_train, preds)\n",
    "print(\"Best Parameters:\", clf.best_params_)\n",
    "print(f\"Training F1: {f1:.4f}\") # This is the \"Training F1\"\n",
    "print(f\"Validation F1: {clf.best_score_:.4f}\") # This is the Validation F1 from the cross-validation procedure in GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37655547-ccc7-439c-86ca-9b0f6ccd36ae",
   "metadata": {},
   "source": [
    "# Exercise 4: Model Assessment\n",
    "\n",
    "Take the best and hyperparameters found from the prior exercises, refit the model and evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f8c64-014e-45bb-a926-7a004697dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02ab98-a420-4ce7-a30a-0a7dff47e98d",
   "metadata": {},
   "source": [
    "# Extra Exercise: Plot the data in two dimensions\n",
    "\n",
    "For this exercise, you should apply PCA to the data to reduce to two dimensions to plot the data. Remeber to apply PCA, you need to 1) apply the StandardScalar and 2) apply PCA. Because our dataset has missing values, you will need to apply the SimpleImputer before applying the StandardScalar.\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c9a0f-2209-4653-913e-324af6cecfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "####################################################\n",
    "# STEP 1\n",
    "\n",
    "# Write code to use SimpleImputer here. (See documentation)\n",
    "\n",
    "imputer = SimpleImputer(...) # MODIFY THIS LINE (use mean)\n",
    "\n",
    "X_train_impute = imputer.fit_transform(X_train)\n",
    "X_test_impute = imputer.transform(X_test)\n",
    "\n",
    "####################################################\n",
    "\n",
    "\n",
    "####################################################\n",
    "# STEP 2\n",
    "# Write code to use SimpleImputer here. (See documentation)\n",
    "\n",
    "scaler = # Instantiate StandardScalar Here\n",
    "\n",
    "X_train_impute_scale = scaler.fit_transform(X_train_impute)\n",
    "X_test_impute_scale = scaler.transform(X_test_impute)\n",
    "\n",
    "####################################################\n",
    "\n",
    "\n",
    "####################################################\n",
    "# STEP 3\n",
    "# Write code to use SimpleImputer here. (See documentation)\n",
    "\n",
    "pca = # Instantiate PCA Here (Make sure you set n_components = 2)\n",
    "\n",
    "X_train_impute_scale_pca = pca.fit_transform(X_train_impute_scale)\n",
    "X_test_impute_scale_pca = pca.transform(X_test_impute_scale)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab56d0e-de8e-4f98-8205-f33e74e661dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# DO NOT Change this cell!!!\n",
    "\n",
    "y_train2 = np.array(y_train)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [1, 0]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = y_train2 == target\n",
    "    ax.scatter(X_train_impute_scale_pca[indicesToKeep,0]\n",
    "               , X_train_impute_scale_pca[indicesToKeep,1]\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa659a-d5ce-4cce-9138-17056133ff36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
