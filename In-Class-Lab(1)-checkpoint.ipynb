{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaaf686b-2b77-479b-b380-85b8be06c0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTES\n",
    "\n",
    "# Questions?\n",
    "\n",
    "# What is a fold? \n",
    "# - Folds are used in Cross Validation (CV) for model selection\n",
    "# - K-fold Cross Validation (3-Fold CV)\n",
    "# 3-Fold CV is like running 3 experiments\n",
    "# Take all the folds except for one (if we have 6 comments, and split it into 3 folds, 2 comments each)\n",
    "# F1 + F2 = Training, F3 = Test, Score = .7\n",
    "# F1 + F3 = Training, F2 = Test, Score = .4\n",
    "# F2 + F3 = Training, F1 = Test, Score = .9\n",
    "# Then we get a score for each model, average them (.667) to get an estimate\n",
    "# Very useful when we dont have a lot of data.\n",
    "# 10 Fold CV is common balance between training and performance\n",
    "# When we have less folds, we have less training data\n",
    "# Models take forever to train sometimes so need to be careful\n",
    "\n",
    "\n",
    "# Macro F1 vs Micro F1\n",
    "# TP_i = True Positives\n",
    "# FP_i = False Positives\n",
    "# FN_i = False Negatives\n",
    "\n",
    "# TP_pos = Number of true positives for the 'pos' class\n",
    "# TP_neg = Number of true positives for the 'neg' class\n",
    "# same for neutral\n",
    "\n",
    "#Then we have Predictions we compare our True Positives to\n",
    "# TP_pos = 2\n",
    "# TP_neg = 1\n",
    "# TP_neu = 2\n",
    "\n",
    "# FP_pos =2\n",
    "# FP_neg = 1\n",
    "# FP_neu = 1\n",
    "\n",
    "# Ground truth is negative but we predicted something else\n",
    "# FN_pos = 1\n",
    "# FN_neg = 2\n",
    "# FN_neu = 1\n",
    "\n",
    "# To calculate macro F1 we need to calc Precision & Recal\n",
    "# Precision:\n",
    "# TP/(TP+FN)\n",
    "\n",
    "# Recall:\n",
    "# TP/(TP+FN)\n",
    "\n",
    "# F1\n",
    "# 2 * Precision * Recall\n",
    "\n",
    "#Macro F1 = Average / mean(Precision,Recall,F1)\n",
    "#Micro F1 = \n",
    "# One of these is better in multiclass situations\n",
    "# What about accuracy?\n",
    "# Accuracy is not good here because if we had 1002 cases and got 1000 pos correct (but the 2 wrong is 1 neg and 1 neu)\n",
    "# then we have 1000/1002 accuracy when we dont have good accuracy when we look at each individual column\n",
    "\n",
    "# Unigrams and Bigrams\n",
    "Data1 = 'The cat sucks'\n",
    "Data2 = 'The dog is great great great!!!'\n",
    "Data3 = 'I really love dogs'\n",
    "Data4 = 'no cats allowed'\n",
    "\n",
    "#Bag-of-Words\n",
    "BoW = ['The','cat','sucks','dog','is','great','I','really','love','no','cats','allowed','dogs']\n",
    "# Unigrams = Single Words\n",
    "# Bigrams = pairs of words\n",
    "# ngram_range(1,2) uses unigrams and bigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f099922-aa79-4b26-a265-e5186dbe764b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'dog', 'is', 'great', 'great', 'great']\n",
      "['The', 'dog', 'is', 'great', 'great', 'great!!!']\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer uses re to find things\n",
    "# Special characters\n",
    "import re\n",
    "\n",
    "pat = r'\\w+'\n",
    "\n",
    "print(re.findall(pat, Data2))\n",
    "print(Data2.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77deaf5f-4945-4622-80a4-a17b0cc88e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Generation, i.e., converting data into a matrix of numbers\n",
    "\n",
    "## DictVectorizer\n",
    "\n",
    "# Way of converting tabluar data into a matrix of numbers\n",
    "\n",
    "\n",
    "## CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Convert a count of data into a matrix of numbers\n",
    "vec = CountVectorizer()\n",
    "\n",
    "## Custom (using numpy)\n",
    "\n",
    "# General Machine Learning Pipeline\n",
    "\n",
    "# Data\n",
    "# Convert into a matrix\n",
    "# Split the data into train,dev,test\n",
    "# Test many things on the dev to find the best model (GridSearchCV) # Model Selection\n",
    "# Evaluate on the test data #Model Assesment\n",
    "\n",
    "# Model Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53382ea-8ef5-4dc4-8c97-4ee0a87a777e",
   "metadata": {},
   "source": [
    "# Exercise 1: Understand your data\n",
    "\n",
    "Access to safe drinking-water is essential to health, a basic human right and a component of effective policy for health protection. This is important as a health and development issue at a national, regional and local level. In some regions, it has been shown that investments in water supply and sanitation can yield a net economic benefit, since the reductions in adverse health effects and health care costs outweigh the costs of undertaking the interventions. Yet, water drinkability is measured by multiple factors, many of which can not be used alone. Hence, this exercise we will develop methods of predicting whether water is drinkable based on variuos measured characteristics.\n",
    "\n",
    "**YOUR TASK:** The first step is to **understand your data**. For each column in the dataset, label whether it is a categorical, ordinal, binary, or numeric value.\n",
    "\n",
    "1. ph: pH of 1. water (0 to 14).\n",
    "    1. Data Type: numeric (could it also be ordinal?)\n",
    "2. Hardness: Capacity of water to precipitate soap in mg/L.\n",
    "    1. Data Type: numeric\n",
    "3. Solids: Total dissolved solids in ppm.\n",
    "    1. Data Type: numeric\n",
    "4. Chloramines: Amount of Chloramines in ppm.\n",
    "    1. Data Type: Numeric\n",
    "5. Sulfate: Amount of Sulfates dissolved in mg/L.\n",
    "    1. Data Type: numeric\n",
    "6. Conductivity: Electrical conductivity of water in μS/cm.\n",
    "    1. Data Type: numeric\n",
    "7. Organic_carbon: Amount of organic carbon in ppm.\n",
    "    1. Data Type: numeric\n",
    "8. Trihalomethanes: Amount of Trihalomethanes in μg/L.\n",
    "    1. Data Type: numeric\n",
    "9. Turbidity: Measure of light emiting property of water in NTU.\n",
    "    1. Data Type:  numeric\n",
    "10. Potability: Indicates if water is safe for human consumption. Potable (1) and Not potable (0) (This is what you will predict).\n",
    "    1. Data Type:  binary / categorical\n",
    "    \n",
    "**TIME**: 10 Minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c9bba-8a41-4843-b873-51586e6fd3d8",
   "metadata": {},
   "source": [
    "# Exercise 2: Create Splits and Explore your data\n",
    "\n",
    "After splitting the dataset and converting the data into a matrix, answer the following questions:\n",
    "1. How many **columns** does the **training data** have?\n",
    "2. How many **rows** does the **training data** have?\n",
    "3. How many **columns** does the **test data** have?\n",
    "4. How many **rows** does the **test data** have?\n",
    "5. What is the max value in each column of the **training data**?\n",
    "6. What is the min value in each column of the **training data**?\n",
    "7. What is the mean value in each column of the **training data**?\n",
    "\n",
    "**TIME**: 10 Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73d1dea7-cead-41fa-880f-256a207a5c62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import csv\n",
    "\n",
    "X_dict = []\n",
    "y = []\n",
    "with open('water_potability.csv') as iFile:\n",
    "    iCSV = csv.reader(iFile, delimiter=',')\n",
    "    next(iCSV)\n",
    "    # ph,Hardness,Solids,Chloramines,Sulfate,Conductivity,Organic_carbon,Trihalomethanes,Turbidity,Potability\n",
    "    for row in iCSV:\n",
    "        data = {}\n",
    "        data['ph'] = float(row[0])\n",
    "        data['Hardness'] = float(row[1])\n",
    "        data['Solids'] = float(row[2])\n",
    "        data['Chloramines'] = float(row[3])\n",
    "        data['Sulfate'] = float(row[4])\n",
    "        data['Conductivity'] = float(row[5])\n",
    "        data['Organic_carbon'] = float(row[6])\n",
    "        data['Trihalomethanes'] = float(row[7])\n",
    "        data['Turbidity'] = float(row[8])\n",
    "        X_dict.append(data)\n",
    "        y.append(int(row[-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165ce519-afa6-4c6a-b2e8-037e49c433a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ph': 0.0,\n",
       " 'Hardness': 204.8904554713363,\n",
       " 'Solids': 20791.318980747026,\n",
       " 'Chloramines': 7.300211873184757,\n",
       " 'Sulfate': 368.51644134980336,\n",
       " 'Conductivity': 564.3086541722439,\n",
       " 'Organic_carbon': 10.3797830780847,\n",
       " 'Trihalomethanes': 86.9909704615088,\n",
       " 'Turbidity': 2.9631353806316407}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e1ab9ba-1a44-4b7d-99ac-6c395fb21d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce12ac47-3a3a-467f-983c-5dc36db80db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "######################\n",
    "# Split dataset into a training and testing portion here.\n",
    "\n",
    "X_train_dict, X_test_dict, y_train, y_test = train_test_split(X_dict, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b956315-3e6e-4b97-b48d-9d0cf7cd619d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell creates the matrix from each list of dictionaries.  You do NOT need to edit this.\n",
    "vec = DictVectorizer()\n",
    "\n",
    "X_train = vec.fit_transform(X_train_dict)\n",
    "X_test = vec.transform(X_test_dict) #Dont run fit on a test set because it can mess up columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "597f89e9-3003-48bf-b5a9-e9795bb841e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2620, 9) (656, 9)\n",
      "(3276,)\n",
      "[[1.31270000e+01 7.53342620e+02 3.23124000e+02 2.83000000e+01\n",
      "  6.12271960e+04 4.81030642e+02 1.24000000e+02 6.73900000e+00\n",
      "  1.40000000e+01]]\n",
      "[[3.52000000e-01 1.81483754e+02 7.34922337e+01 4.37189861e+00\n",
      "  7.28750830e+02 0.00000000e+00 0.00000000e+00 1.49220662e+00\n",
      "  0.00000000e+00]]\n",
      "[[7.10786735e+00 4.26420949e+02 1.96448911e+02 1.42518601e+01\n",
      "  2.20665978e+04 2.53296342e+02 6.31386807e+01 3.95990306e+00\n",
      "  6.02021778e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "import numpy as np\n",
    "y = np.array(y)\n",
    "print(y.shape)\n",
    "\n",
    "print(X_train.max(axis=0).toarray())\n",
    "print(X_train.min(axis=0).toarray()) # Zeros mean there are missing values in the data\n",
    "print(X_train.mean(axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e8a4c-529c-4e50-b941-e5184f46bf05",
   "metadata": {},
   "source": [
    "# Exercise 3: Model Selection\n",
    "\n",
    "The data has been split into a training and test dataset. Now, your task is to train a machine learning classifier on the dataset. Specifically, you need to train and evaluate 3 different machine learning models: RandomForestClassifier, SVC, and LogisticRegression. For each classifier, you MUST gridsearch on two of its parameters. You can find the parameters for each model below:\n",
    "\n",
    "1. RandomForestClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "    1. I recommend n_estimators and criterion\n",
    "2. SVC: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    1. I recommend C and kernel\n",
    "3. LogisticRegression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "    1. I recommend C and fit_intercept\n",
    "    \n",
    "Report the training F1 and cross-validation F1 in the following table:\n",
    "\n",
    "|Model| Training F1 | Validation F1|\n",
    "|----|----|----|\n",
    "|Random Forest | SCORE HERE | SCORE HERE |\n",
    "|SVC | SCORE HERE | SCORE HERE |\n",
    "|LogisticRegression | SCORE HERE | SCORE HERE |\n",
    "\n",
    "Finally, answer the questions: Is the training F1 higher then the validation F1? Why or why not?\n",
    "\n",
    "**TIME**: 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70820720-21cc-4290-9197-2d788fb2efe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [656, 2620]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     16\u001b[0m preds \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[1;32m---> 17\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, preds)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, clf\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# This is the \"Training F1\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1238\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1071\u001b[0m     {\n\u001b[0;32m   1072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1096\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1097\u001b[0m ):\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \n\u001b[0;32m   1100\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fbeta_score(\n\u001b[0;32m   1239\u001b[0m         y_true,\n\u001b[0;32m   1240\u001b[0m         y_pred,\n\u001b[0;32m   1241\u001b[0m         beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1242\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1243\u001b[0m         pos_label\u001b[38;5;241m=\u001b[39mpos_label,\n\u001b[0;32m   1244\u001b[0m         average\u001b[38;5;241m=\u001b[39maverage,\n\u001b[0;32m   1245\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1246\u001b[0m         zero_division\u001b[38;5;241m=\u001b[39mzero_division,\n\u001b[0;32m   1247\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    186\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1411\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1251\u001b[0m     {\n\u001b[0;32m   1252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1279\u001b[0m ):\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m \n\u001b[0;32m   1282\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;124;03m    0.38...\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1411\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1412\u001b[0m         y_true,\n\u001b[0;32m   1413\u001b[0m         y_pred,\n\u001b[0;32m   1414\u001b[0m         beta\u001b[38;5;241m=\u001b[39mbeta,\n\u001b[0;32m   1415\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1416\u001b[0m         pos_label\u001b[38;5;241m=\u001b[39mpos_label,\n\u001b[0;32m   1417\u001b[0m         average\u001b[38;5;241m=\u001b[39maverage,\n\u001b[0;32m   1418\u001b[0m         warn_for\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-score\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[0;32m   1419\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1420\u001b[0m         zero_division\u001b[38;5;241m=\u001b[39mzero_division,\n\u001b[0;32m   1421\u001b[0m     )\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    186\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1721\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1563\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1564\u001b[0m \n\u001b[0;32m   1565\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1719\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m zero_division_value \u001b[38;5;241m=\u001b[39m _check_zero_division(zero_division)\n\u001b[1;32m-> 1721\u001b[0m labels \u001b[38;5;241m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1499\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1499\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [656, 2620]"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression # Best to stay with one of these and add a lot of features, linearSVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "params = {}\n",
    "\n",
    "base_clf = RandomForestClassifier() # The classifier (e.g., LogisticRegression) goes here.\n",
    "\n",
    "clf = GridSearchCV(base_clf, params, cv = 5, scoring='f1')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_train)\n",
    "f1 = f1_score(y_test, preds)\n",
    "print(\"Best Parameters:\", clf.best_params_)\n",
    "print(f\"Training F1: {f1:.4f}\") # This is the \"Training F1\"\n",
    "print(f\"Validation F1: {clf.best_score_:.4f}\") # This is the Validation F1 from the cross-validation procedure in GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb41d1-cb78-4580-b146-5f42588ce695",
   "metadata": {},
   "source": [
    "# Exercise 5: Model Assessment\n",
    "\n",
    "Now that you have found the best model, we need to measure how the model will perform on new unseen data. We will use the extrated test data as our \"unseen\" data.\n",
    "\n",
    "**YOUR TASK**: Retrain the best classifier using the best parameters found via GridSearchCV. Note that GridSearchCV is not needed here. Do the final test results match the cross-validation results found via the GridSearchCV method?\n",
    "\n",
    "**TIME**: 10 Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc99c3-ad6a-40cf-a66a-f05ac6a3ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "clf = ... # The classififier (e.g., RandomForestClassifier) goes here\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "precision = precision_score(y_test, preds)\n",
    "recall = recall_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds)\n",
    "\n",
    "print(f\"precision: {precision:.4f}\")\n",
    "print(f\"recall: {recall:.4f}\")\n",
    "print(f\"f1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bd419-79ac-4590-85e9-f435ad2db103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
